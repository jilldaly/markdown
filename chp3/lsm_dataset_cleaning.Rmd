---
title: "MSc Data Science & Analytics: LSM Track Data Cleaning & Exploration"
author: "Jill Daly"
date: "15/10/2018"
output:
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
---

<!-- 
Online information about creating technical documents with RMarkdown and knitr:
https://bookdown.org/
https://bookdown.org/yihui/bookdown/
https://rmarkdown.rstudio.com/
https://github.com/rstudio/rmarkdown
https://rmarkdown.rstudio.com/pdf_document_format
http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf
https://yihui.name/knitr/options/?version=1.2.830&mode=desktop#code-chunk
https://stackoverflow.com/questions/12546365/subfigures-or-subcaptions-with-knitr/49086985#49086985
https://rmarkdown.rstudio.com/articles_report_from_r_script.html
-->

<!-- TODO: Create separate .tex files to deal with styling issues, eg. toc on same page as title -->
\newpage 
\tableofcontents 
\listoffigures
\listoftables
\newpage


<!--##########################################################################
##############################################################################
##                                                                          ##
##                               DATA WRANGLING                             ##
##                       FOR LOADING & TIDYING DATASET                      ##  
##                                                                          ##
##############################################################################
##############################################################################-->


<!--##########################################################################
##                            SCRIPT CONFIG VALUES                          ##
##                                                                          ##
##          These are the hardcoded values that need to be configured       ##
##          for each data capture                                           ##
##############################################################################-->

```{r config, include=FALSE}
# Modules used by this script
source("InstallImport.R")
source("LoadControllerLogs.R")
source("LoadSensorData.R")
source("EDA.R")

# Required Packages
install_imports(c("tidyverse", "VIM", "kableExtra", "PerformanceAnalytics", "Hmisc", "outliers")) 


# Hardcoded paths used in this script
DATA_PATH <- "../../../data"
DC2_LOGS_PATH <- paste(DATA_PATH, "Data_Capture_2_ControllerLogs", sep = "/")
DC2_ALL_LOGS <- paste(DC2_LOGS_PATH, '.', sep = "/")
DC2_EXCEL <- paste(DATA_PATH, "Data_Capture_2_Raw.xlsx", sep = "/")
```


<!-- FIGURES FROM SECTIONS CAN BE SAVED TO DISK FOR THE FINAL THESIS DOCUMENT -->

\newpage
# Introduction  

This thesis research question stems from research for a separate Master’s thesis project (Catenazzo, 2018), which was completed in association with the Tyndall National Institute. That project (ibid) examined data generated from a prototype Wireless Sensor Network (WSN) for the purpose of designing a Predictive Maintenance (PdM) pipeline for a manufacturing production environment. The problem of how to accurately identify useful data observations from experiments, conducted as part of the Tyndall’s prototype, will be addressed by research for this MSc thesis. Isolation of this useful data will improve cost and accuracy of the final PdM pipeline for its designated manufacturing pipeline.

The following document is a draft report, which will feed into the final thesis document. The aim of this draft report is to provide transparency regarding steps taken while cleaning the data. By outlining the steps taken, this project aims to meet with open science reproducibility standards (add ref).

\newpage
# Data Capture Experiment - Lit Review 

Catenazzo’s work (ibid) analysed results from two experiments. The first experiment involved a static application in a laboratory setting. The results from this experiment led to the second experiment, this time involving a prototype of a manufacturing production line.

It was while examining results from this second prototype experiment (ibid), that a gap in the research was identified. A problem emerged of how to effectively isolate data observations from specific sections of the track constructed for the prototype experiment. This MSc thesis seeks a solution to this problem by identifying the state of the vehicle on the track from the data observations generated by the experiment. 

Isolating the data is useful because it represents sensor readings taken from straight paths along the closed loop prototype. The prototype was intended to mimic a production environment, however in reality it needed to transition the vehicle on shuttles between each straight path section. This was necessary in order to increase the distance traveled by the vehicle. This shuttle will not exist in production, and the data generated from the transition to and from the shuttle, as well as the shuttle readings, need to be omitted from the dataset. 

The isolated data observations will be labelled by a supervised learning classification model, which will be constructed as part of this MSc thesis. This labelled data will feed into the design and implementation of a PdM pipeline in a manufacturing production environment. Successfully omitting the shuttle and transition data, and including the straight path data will improve the quality of the data, ultimately improving the accuracy of a PdM pipeline which detects faults of the system. 

Before building the classification model, data cleaning and exploratory analysis of the data produced by the prototype experiment is required.


** Lookup – who are MagenMotion.
The prototype data capture was performed using MagneMotion’s Linear Synchronous Motor (LSM) driven assembly system (Ranky, 2007).

The prototype data capture was performed using QuickStick, which is MagneMotion’s patented LSM system. This system propels and controls each vehicle independently by interacting with a permanent magnet array mounted to each vehicle (ibid). 

[ TODO – Show figure from paper/display QuickStick drawing/design ]

The core technology of an LSM is the linear electric motor. This motor can convert electrical energy directly into linear motion. QuickStick LSM is a 1m long, linear motor module with embedded position sensors and control software. QuickStick controls the velocity and acceleration of the system. Each node controller can handle 320 QuickSticks on 8 paths. A production environment could host many node controllers, allowing for multiple vehicles to be tracked and managed. 

For this prototype data capture, the experiment design included 6 QuickStick 1m long modules in a closed loop. These QuickStick modules were joined to form a “Straight Path”. There were two straight paths in the prototype loop, with shuttles bridging the gap between the straight paths. 

[ TODO – Insert Path Diagram from Donato’s thesis, and reference ]



<!--##########################################################################
##                             LOAD CONTROLLER LOGS                         ##
##                                                                          ##
##          Intentionally loading these as separate entities,               ##
##          so that we can analyse the data in isolation,                   ##
##          and find corrlataions                                           ##
##############################################################################-->

\newpage

# Second Data Capture Controller Logs  

The controler logs document commands issued to the LSM vehicle during the prototype experiment. In a production environment, there could be many vehicles on many productions lines, all running in parallel. The controller would manage each of these vehicles, tracking where each vehicle is positioned on each line. The secidn data capture prototype experiment 

```{r logs_config, fig.cap="Loading Controller Log Files"}
# These columns are specific to Data Capture May 2018
factor_cols <- list("Vehicle", "Path", "DstPath", 'Destination', 
                    "Cmd", "Sig", "Obs", "Jam", "Sus", "PID")
missed_file <- paste(DC2_LOGS_PATH, "Acc_1_Vel_4_Test2", sep = "/")
```


```{r loadlogs}
# Create a list of data frames for the controller log files 
controller_logs <- load_controller_logs(DC2_ALL_LOGS, factor_cols, missed_file)
```


<!-- 
# kable Api details: https://rdrr.io/cran/knitr/man/kable.html
# kableExtra ides:
# https://www.r-exercises.com/2018/02/05/how-to-use-kableextra-and-rmarkdown-to-create-tables-in-pdf-documents/

# latex_options of hold_postion and scale_down are very important. 
# Without these, the table is not readable and outside of the flow 
# of the data. See:
# https://stackoverflow.com/questions/49550731/knitr-kable-table-moving-to-the-end-of-page-when-adding-caption-figure-label
# https://stackoverflow.com/questions/49044753/scale-kable-table-to-fit-page-width
-->

```{r logfilecount, echo=FALSE}
# Get the full, raw observation count
cl_dets <- map_df(names(controller_logs), sensor_summary, dfl = controller_logs)
cl_n_count <- sum(cl_dets$r_count)
```

The total observation count for the all controller logs is `r cl_n_count`.
 

Here is a summary of a single log file 

```{r summlogfiles, echo=FALSE}
cl_summ <- summary(controller_logs[[1]])
knitr::kable(cl_summ, caption = "Controller Log File - Summary") %>% 
  kable_styling(latex_options=c("scale_down", "hold_position"))
```


Here is a summary of contoller log file dimensions for each run:

```{r logfiledetails, echo=FALSE}
# Visualise NAs, and get summary of the data here, pre-dropping of NA cols
knitr::kable(cl_dets, caption = "Controller Datasets") %>% 
  kable_styling(latex_options=c("hold_position")) 
```


This is a sample controller Log file:

```{r strlogfiles, echo=FALSE}
cl_str <- capture.output(str(controller_logs[[1]]))
knitr::kable(cl_str, 
             caption = "Sample Controller Log File - Structure", 
             col.names = "Log File Structure") %>%
  kable_styling(latex_options=c("scale_down", "hold_position")) 
```


\newpage

## Controller Logs Missing Data

There is only one log file with missing data, as is demonstrated with the data below. This is because the log file `Acc_1_Vel_4_Test2` has a extra columns included, which were added after the data capture. They can be dicounted as missing data. These values will form part of the analysis for joining the Sensor and Controller datasets, so that they are part of a single dataset. This final dataset will then be presented for the data preprocessing and model construction in a later section. 

```{r log_nas, echo=FALSE}
cl_nas <- capture.output(str(map_dfc(controller_logs, function(x) { sum(!complete.cases(x))} )))
knitr::kable(cl_nas, caption = "Controller Missing Data - Summary", 
             col.names = "Missing Data") %>%
  kable_styling(latex_options=c("hold_position"))
```



\newpage

### TODO - Discuss meaning of each column
### TODO - Plot Bar charts for Logs
### TODO - Add Time Column to Controller Logs
### TODO - Create Label Column from first 3 columns
### TODO - Add section on QuickStick


\newpage

# Data Capture Sensor Data Files

TODO - Give description of LSM Vehicle, and Track
       Give explanation of each run, and number of sensors
       Re-iterate that 2nd Data Capture is the only one of interest here.
       Explain that there are 3 axes per sensor
       Include Image of Sensor Orientation, and also description of MPU925 Sensor




## Hardcoded Values

These hardcoded values are only for the May 2018 Data Capture. There are two main objective to placing these hardcoded items outside of the functions and logic. Firstly, they provide transparency so that readers can discern decisions and secondly, because the values are easily edited, without impacting/breaking the code. 
 
```{r sensorheader}

# Pre-defined header for the raw sensor data
sensor_header = c("IMU1_X",	"IMU1_Y",	"IMU1_Z",	
                  "IMU2_X",	"IMU2_Y",	"IMU2_Z",
                  "IMU3_X",	"IMU3_Y",	"IMU3_Z",
                  "IMU4_X",	"IMU4_Y",	"IMU4_Z",
                  "IMU5_X",	"IMU5_Y",	"IMU5_Z",
                  "IMU6_X",	"IMU6_Y",	"IMU6_Z",
                  "IMU7_X",	"IMU7_Y",	"IMU7_Z",
                  "IMU8_X",	"IMU8_Y",	"IMU8_Z",
                  "IMU9_X",	"IMU9_Y",	"IMU9_Z",	
                  "IMU10_X", "IMU10_Y",	"IMU10_Z",
                  "IMU11_X", "IMU11_Y",	"IMU11_Z",	
                  "IMU12_X", "IMU12_Y",	"IMU12_Z",
                  "Timestamp", "Delta")


# Magnetometer Data Column 1,2 Swap 
mag_col_swap = c("IMU1_Y", "IMU1_X", "IMU1_Z",	
                 "IMU2_Y", "IMU2_X", "IMU2_Z",
                 "IMU3_Y", "IMU3_X", "IMU3_Z",
                 "IMU4_Y", "IMU4_X", "IMU4_Z",
                 "IMU5_Y", "IMU5_X", "IMU5_Z",
                 "IMU6_Y", "IMU6_X", "IMU6_Z",
                 "IMU7_Y", "IMU7_X", "IMU7_Z",
                 "IMU8_Y", "IMU8_X", "IMU8_Z",
                 "IMU9_Y", "IMU9_X", "IMU9_Z",	
                 "IMU10_Y",	"IMU10_X", "IMU10_Z",
                 "IMU11_Y",	"IMU11_X", "IMU11_Z",	
                 "IMU12_Y",	"IMU12_X", "IMU12_Z",
                 "Timestamp",	"Delta")


# Magnetometer Negate Col3 (Z Axes)
mag_z_negate = negate_hdr <- c("IMU1_Z", "IMU2_Z", "IMU3_Z", 
                               "IMU4_Z", "IMU5_Z", "IMU6_Z", 
                               "IMU7_Z", "IMU8_Z", "IMU9_Z", 
                               "IMU10_Z", "IMU11_Z", "IMU12_Z")


# This header swap is to account for orientation of the sensor.
swap_position_header = c("IMU1_Y", "IMU1_X", "IMU1_Z",	
                         "IMU2_X", "IMU2_Y", "IMU2_Z", 
                         "IMU3_X", "IMU3_Y", "IMU3_Z",
                         "IMU4_X", "IMU4_Y", "IMU4_Z",
                         "IMU5_X", "IMU5_Y", "IMU5_Z",
                         "IMU6_X", "IMU6_Y", "IMU6_Z",
                         "IMU7_Z", "IMU7_X", "IMU7_Y",
                         "IMU8_Y", "IMU8_X", "IMU8_Z",
                         "IMU9_Z", "IMU9_X", "IMU9_Y",	
                         "IMU10_X", "IMU10_Y", "IMU10_Z",
                         "IMU11_X", "IMU11_Y", "IMU11_Z",	
                         "IMU12_X",	"IMU12_Y", "IMU12_Z",
                         "Timestamp",	"Delta")


# Axes to negate (using the post converted axis (already adjusted for orientation)
negate_hdr <- c("IMU1_Y", "IMU3_Y", "IMU3_Z", 
                "IMU8_Z", "IMU9_Y", "IMU9_Z", 
                "IMU12_Y", "IMU12_Z")


# There were 9 unique accel/vel settings 
# (10th run re-used setting, with platform off)
run_settings <- c("a_1_v_0_2", "a_1_v_0_03", "a_1_v_0_4", 
                  "a_1_v_0_06", "a_1_v_0_0135", "a_2_v_0_2", 
                  "a_2_v_0_4", "a_2_v_0_06", "a_2_v_0_6" )


# IMU Sensor Positions:
s_pos_df <- data.frame(t(c("IMU1_POS"="FRONT_SURFACE", 
                         "IMU3_POS"="TOP_SURFACE_WITH_PALETTE",
                         "IMU7_POS"="FRONT_SURFACE", 
                         "IMU8_POS"="FRONT_SURFACE", 
                         "IMU9_POS"="SIDE_SURFACE", 
                         "IMU11_POS"="TOP_SURFACE", 
                         "IMU12_POS"="TOP_SURFACE")))
```



<!--##########################################################################
#                     LOAD THE DATA FILE FOR SENSOR DATA                     #
##############################################################################-->

When loading the sensor data, each run dataset is loaded as a dataframe, and then added to the `sensor_data` list. Due to the fact that there are acceleration and magnetometer  sensor measurements, there are 20 dataframes in the `sensor_data` list. 
```{r load_sensor_files}
# Create a list of data frames for the magnetic and vibration sensor data 
sensor_data <- load_sensor_raw(DC2_EXCEL, sensor_header)
```


```{r sensorfilecount, include=FALSE}
# Get the full, raw observation count
s_dets <- map_df(names(sensor_data), sensor_summary, dfl = sensor_data)
s_n_count <- sum(s_dets$r_count)
```

The total observation count for sensor data is `r s_n_count`. This is a summary of the raw sensor data file dimension. 

```{r sensorfiledetails, echo=FALSE}
# Get summary of the data here, pre-dropping of NA cols
knitr::kable(s_dets, caption = "Raw Sensor Datasets") %>% 
  kable_styling(latex_options=c("hold_position"))
```
 

 


\newpage

## Controller Logs Missing Data



 
 
As a result of different sensor orientation, it is necessary to adjust the axis columns to their true position in each data frame. Here the variables from the `Hardcoded Values` section are utilised, when passed to the `convert_axis` function.

```{r loadsensor}
# Adjust the XYZ order of axes to account for sensor orientation
# Adjust for gravity calibration 
sensor_data <- convert_axis(sensor_data, mag_col_swap, mag_z_negate, 
                            swap_position_header, negate_hdr)
```


```{r include=FALSE}
agg_data_caption <- paste(names(sensor_data)[[3]], " and ", names(sensor_data)[[20]], " Sensor Datasets")
```


```{r aggrsensorfiles-pre_3, echo=FALSE, fig.cap=agg_data_caption}
sensor_aggr(sensor_data[[3]])
sensor_aggr(sensor_data[[20]])
```


Here are sample `aggr` plots displaying the amount of missing sensor data from the a sample raw dataset. This is due to some sensors not funtioning during the data capture, due to a number of technical issues.  The second plot represents the rig where the platform was not present, effectively changing the vehicle payload. 


It is evident from the missing data plots that there are a number of columns that have little or no data. Due to the fact that these are missing as a result of technical facults, these columns can be dropped. 



```{r sensor-removenas}
# Drop Columns & Rows with only NAs, as these are empty variables/rows
# due to mechanical issues and should not be included
sensor_data <- drop_na_only_cols(sensor_data, .15)
```





```{r aggrsensorfiles-post, echo=FALSE, fig.cap=names(sensor_data)[[3]], fig.height=4, fig.width=4}
sensor_aggr(sensor_data[[3]])
```

Here is the `aggr` plot for run `r names(sensor_data)[[3]]` after the un-recorded sensors have been removed. This allows for more meaningful analysis of missing sensor data, after columns with missing data above the 85% threshold were removed. 


\newpage

## Adding Columns to Sensor Dataset
In order to facilitate merging of all acceleration and magnetometer dataframes, the acceleration, velocity and platform values are added to each individual dataset.
```{r add_setting_cols}
# Add Velocity and Acceleration columns
sensor_data <- sensor_vel_accel_list(sensor_data, run_settings)

```


```{r sensorfilecount-post, include=FALSE}
# Get the full, raw observation count
s_dets <- map_df(names(sensor_data), sensor_summary, dfl = sensor_data)
s_n_count <- sum(s_dets$r_count)
```
  
  
This is a summary of the sensor data file dimensions (after dropping columns with 85% or more NAs, adding now columns)
```{r sensorfiledetails-post, echo=FALSE}
# Get summary of the data here, pre-dropping of NA cols
knitr::kable(s_dets, caption = "Post Cleaning Sensor Datasets") %>% 
  kable_styling(latex_options=c("hold_position"))
```

The total observation count for the cleaned sensor data remains `r s_n_count`.




